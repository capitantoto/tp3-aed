---
title: "TP-3"
output: html_document
author: Barrera Borla, Berros, Duarte
---
# Preliminares

Cargamos las librerías del *pulcriverso*.

```{r inicialización}
library(tidyverse, quietly = TRUE)
library(glue)
```

Leemos los datos y realizamos, construimos un `data-frame` con nombres significativos y tipos adecuados a cada variable para, en base a él,  realizar el scatter-plot de las variables elevación y temperatura anual.

```{r ingreso de datos, cache=T}
df <- read_csv(
  file = "ortann.csv",
  col_types = cols(
    station = col_character(),
    latitude = col_double(),
    longitude = col_double(),
    elevation = col_integer(),
    tann = col_double()
  ))

nombres_castellano <- c(
  "station" = "estacion",
  "latitude" = "latitud",
  "longitude" = "longitud",
  "elevation" = "elevacion",
  "tann" = "temp_anual")

df <- plyr::rename(df, nombres_castellano)

df %>%
  ggplot(aes(x = elevacion, y = temp_anual)) +
  geom_point() -> fig1

ggsave("fig1_dispersion.png", fig1)
fig1
```

Siguiendo el criterio de trabajos precedentes, quitamos dos outliers del dataset.

```{r eliminación de atípicos}
df <- filter(df, elevacion < 1500)
```
# Estimación No Paramétrica

## Implementación del Estimador de Nadaraya Watson

### Definición de los Núcleos

Definimos los núcleos para la estimación no paramétrica.

```{r núcleos}
indicadora <- function(z) { ifelse(abs(z) <= 1, 1, 0) }

nucleos <- list(
    'uniforme' = function(z) { indicadora(z) * (1/2) },
    'triangular' = function(z) { indicadora(z) * (1 - abs(z)) },
    'epanechnikov' = function(z) { indicadora(z) * (3/4) * (1 - z^2) },
    'gaussiano' = function(z) { (1/sqrt(2 * pi)) * exp(-z^2/2) }
)
```

Y los graficamos para comprobar que actúan como esperamos.

```{r gráfico de los núcleos, cache=T}
rango <- seq(-1.1, 1.1, by = 0.01)

tibble(
  x = rep(rango, each = length(nucleos)),
  nombre_nucleo = rep(names(nucleos), length(rango)),
  y =  map2_dbl(nombre_nucleo, x, ~nucleos[[.x]](.y))
) %>%
  ggplot(aes(x, y, color = nombre_nucleo)) +
    geom_line() +
    coord_fixed() -> fig2

ggsave("fig2_nucleos.png", fig2, width = 6, height = 3.5)
fig2
```

### Estimador de la Densidad

```{r}
estimar_densidad <- function(X, nombre_nucleo, h, x) {
    n <- length(X)
    nucleo <- nucleos[[nombre_nucleo]]   
    sum(nucleo((X - x) / h)) / (2 * n * h)
}
```

Estimamos la densidad con todos los nucleos definidos sobre una primera grilla que sencillamente recorre los valores entre el máximo y el mínimo del conjunto de datos. 

En este caso, el bucle `for` recorre la lista de valores tentativos de `h` provista y luego de realizar las estimaciones con ese `h` para cada núcleo produce un gráfico que sintetiza la información obtenida. 

```{r}
grilla <- min(df$elevacion):max(df$elevacion)

for (h in c(50, 200, 400)) {
    tibble(
        x = rep(grilla, length(names(nucleos))),
        nucleo = rep(names(nucleos), each = length(grilla)),
        densidad = map2_dbl(x, nucleo, function(x, nucleo) {
            estimar_densidad(df$elevacion, nucleo, h, x)
        })
    ) %>%
      ggplot() +
        aes(x, y = densidad, color = nucleo) +
        geom_line() +
        labs(title = glue('h = {h}')) +
        theme(
            axis.ticks.y = element_blank(),
            axis.text.y = element_blank()
        ) -> fig

    ggsave(glue('fig3_densidad_con_h_{h}.png'), fig, width = 7, height = 4)
    print(fig)
}
```

### Implementación de la Regresión No Paramétrica

Elegimos el núcleo de Epanechnikov. Vamos a realizar la regresión no paramétrica con el método Nadaraya-Watson.


Además de definir este estimador, ensayamos también una función currificada `construir_estimador_de_error_NW_dado_h` que va a ser útil a la hora de realizar la validación del modelo, ya que permite fijar los conjuntos de datos y tomar `h` como única variable, encargándose la función de realizar un proceso de validación cruzada "Validación con un dato fuera" o *LOOCV* por sus siglas inglesas.

El costo del proceso es quizás excesivo pero optamos por usarlo en lugar del *K-Fold* porque garantiza estabilidad en los resultados y eso es fundamental a la hora de aclimatarse con una herramienta como esta, además de que la exigüidad de los datos lo hace posible.

```{r implementación de la regresión no paramétrica}
NUCLEO_ELEGIDO <- 'epanechnikov'

construir_estimador_NW <- function(X, Y, nombre_nucleo, h) {
    function(x) {
        nucleo <- nucleos[[nombre_nucleo]]   
        pesos <- nucleo((X - x) / h)
        weighted.mean(Y, pesos)
    }
}

construir_estimador_de_error_NW_dado_h <- function(X, Y, nombre_nucleo) {
    function(h) {
        estimador_iesimo <- function(i) { construir_estimador_NW(X[-i], Y[-i], nombre_nucleo, h) }
        estimadores <- map(seq_along(X), estimador_iesimo)
        predicciones <- pmap_dbl(
          list(estimador = estimadores, x = X),
          function(estimador, x) { estimador(x) }
        )
        return (mean((Y - predicciones)^2))
    }
}

estimador_de_error_NW_dado_h <-
  construir_estimador_de_error_NW_dado_h(df$elevacion, df$temp_anual, NUCLEO_ELEGIDO)
```

#### Buscamos una Referencia sobre el mejor *h*

Intentamos utilizar `optimize` para encontrar el `h` que minimiza el error, pero el resultado varía según el rango que recibe. Sospechamos que la función se queda en una región "amesetada" alrededor del 370 y sólo puede llegar al verdadero mínimo (alrededor de $150$, como veremos más adelante) si restringimos el rango de valores de `h` en el que busca el extremo:
```{r}


resultado_1 <- optimize(estimador_de_error_NW_dado_h, c(-10, 1000))$minimum
resultado_2 <- optimize(estimador_de_error_NW_dado_h, c(-10, 250))$minimum # Rango restringido

print(glue('h con error mínimo según "optimize", sin restringir el rango = {round(resultado_1, 2)}'))
print(glue('h con error mínimo según "optimize", restringiendo el rango = {round(resultado_2, 2)}'))
```

#### Buscamos el mejor *h* con una implementación propia

Vamos a buscar el valor de *h* óptimo con el método Leave-one-out cross validation (LOOCV), que como mencionamos es parte de la implementación de `estimador_de_error_NW_segun_h`.

```{r LOOCV para NW, cache=T }
valores_h <- c(seq(from = 40, to = 1000, by = 50), # Grilla gruesa
               seq(from = 40, to = 350, by = 10), # Grilla más fina en región candidata
               seq(from = 120, to = 190, by = 1)) # Grilla muy fina en el lo más hondo del valle

errores_segun_h <- tibble(
  h = sort(unique(valores_h)),
  error_nw = map_dbl(h, estimador_de_error_NW_dado_h)
)

MEJOR_H <- slice(errores_segun_h, which.min(error_nw))$h

errores_segun_h %>%
  filter(!is.na(error_nw)) %>%
  ggplot(aes(h, error_nw)) +
    geom_point(shape = 39, size = 3) +
    geom_line(alpha = .3) +
    geom_vline(xintercept = MEJOR_H, alpha = .6, linetype = 'dashed', color = 'ForestGreen') +
    annotate('text', x = MEJOR_H + 1, y = 0.9, label = glue('h = {MEJOR_H}'), color = 'ForestGreen') +
    labs(title = glue('Error mínimo del estimador N-W con h = {MEJOR_H}')) +
    ylab('Error cuadrado medio') +
    scale_x_continuous(breaks = c(seq(100, 200, 20), seq(0, 1000, 100))) +
    theme(axis.text.x = element_text(angle = 90),
          axis.ticks.y = element_blank()) -> fig

ggsave('fig4_error_NW_segun_h.png', fig, width = 7, height = 4)
fig

modelo_no_parametrico <- construir_estimador_NW( df$elevacion, df$temp_anual, NUCLEO_ELEGIDO, 155 )
modelo_no_parametrico(5)
```


# Estimación paramétrica

Vamos a comprar nuestro modelo no paramétrico con sus antagonistas paramétricos ajustando varios polinomios por `lm` y obteniendo su error por procedimientos similares. 
Primero vamos a definir una función `polinomio_dados_coeficientes` para poder aprovechar la salida del comando `lm`.

```{r estimaciones por lm, cache=T}

polinomio_dados_coeficientes <- function(coeficientes) {
  polinomio <- function(x) {
    sum(
      pmap_dbl(
        list( i = seq_along(coeficientes), coeficiente = coeficientes ),
        function(i,coeficiente) {coeficiente * x^(i - 1)}
      )
    )
  }
  return(polinomio)
}  

evaluar_modelo_lineal_LOOCV <- function(X,Y,grado){
  estimador_iesimo <- function(i) {
    coeficientes <- coefficients(
      lm(Y~poly(X,degree=grado,raw=T),list(X[-i],Y[-i]))
      )
    return ( polinomio_dados_coeficientes(coeficientes) )

  }
  estimadores <- map(seq_along(X), estimador_iesimo)
  predicciones <- pmap_dbl(
    list(estimador = estimadores, x = X),
    function(estimador, x) { estimador(x) }
  )
  return (mean((Y - predicciones)^2))
}

error_lin <- evaluar_modelo_lineal_LOOCV(df$elevacion,df$temp_anual,1)
error_cuad <- evaluar_modelo_lineal_LOOCV(df$elevacion,df$temp_anual,2)

error_lin
error_cuad
estimador_de_error_NW_dado_h(155)

modelo_lineal <- polinomio_dados_coeficientes (unname ( coefficients (
  lm (temp_anual ~ poly(elevacion, degree = 1,raw=T), list(elevacion = df$elevacion,temp_anual = df$temp_anual))
  ) ) )

modelo_cuadratico <- polinomio_dados_coeficientes (unname ( coefficients (
  lm (temp_anual ~ poly(elevacion, degree = 2,raw=T),list(elevacion = df$elevacion,temp_anual = df$temp_anual))
  ) ) )

modelo_grado_alto <- polinomio_dados_coeficientes (unname ( coefficients (
  lm (temp_anual ~ poly(elevacion, degree = 10,raw=T),list(elevacion = df$elevacion,temp_anual = df$temp_anual))
  ) ) )


error_grado_alto <- evaluar_modelo_lineal_LOOCV(df$elevacion,df$temp_anual,10)
error_grado_alto

```


```{r gráfico encimado de los modelos, cache=T}

rango <- seq(min(df$elevacion),max(df$elevacion), by = 10)

modelos <- list(
  'lineal' = modelo_lineal,
  'cuadrático' = modelo_cuadratico,
  'grado diez' = modelo_grado_alto,
  'no paramétrico' = modelo_no_parametrico
)

tabla_modelos <- tibble(
  x = rep(rango, each = length(modelos)),
  nombre_modelo = rep(names(modelos), length(rango)),
  y =  map2_dbl(nombre_modelo, x, ~modelos[[.x]](.y))
) 
  ggplot(tabla_modelos,aes(x, y)) +
    geom_line(aes(color=nombre_modelo)) +
    geom_point(data = df, aes( x = elevacion, y = temp_anual)) -> figComparacion

ggsave("figComparacion.png", figComparacion, width = 10, height = 10)
figComparacion
```
