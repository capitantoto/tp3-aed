---
title: "TP-3"
output: html_document
author: Barrera Borla, Berros, Duarte
---

Cargamos las librerías del *pulcriverso*.

```{r}
library(tidyverse, quietly = TRUE)
library(glue)
```

Leemos los datos y realizamos, construimos un `data-frame` con nombres significativos y tipos adecuados a cada variable para, en base a él,  realizar el scatter-plot de las variables elevación y temperatura anual.

```{r}
df <- read_csv(
  file = "ortann.csv",
  col_types = cols(
    station = col_character(),
    latitude = col_double(),
    longitude = col_double(),
    elevation = col_integer(),
    tann = col_double()
  ))

nombres_castellano <- c(
  "station" = "estacion",
  "latitude" = "latitud",
  "longitude" = "longitud",
  "elevation" = "elevacion",
  "tann" = "temp_anual")

df <- plyr::rename(df, nombres_castellano)

df %>%
  ggplot(aes(x = elevacion, y = temp_anual)) +
  geom_point() -> fig1

ggsave("fig1_dispersion.png", fig1)
fig1
```

Siguiendo el criterio de trabajos precedentes, quitamos dos outliers del dataset.

```{r}
df <- filter(df, elevacion < 1500)
```

Definimos los núcleos para la estimación no paramétrica.

```{r}
indicadora <- function(z) { ifelse(abs(z) <= 1, 1, 0) }

nucleos <- list(
    'uniforme' = function(z) { indicadora(z) * (1/2) },
    'triangular' = function(z) { indicadora(z) * (1 - abs(z)) },
    'epanechnikov' = function(z) { indicadora(z) * (3/4) * (1 - z^2) },
    'gaussiano' = function(z) { (1/sqrt(2 * pi)) * exp(-z^2/2) }
)
```

Y los graficamos para comprobar que actúan como esperamos.

```{r}
rango <- seq(-1.1, 1.1, by = 0.01)

tibble(
  x = rep(rango, each = length(nucleos)),
  nombre_nucleo = rep(names(nucleos), length(rango)),
  y =  map2_dbl(nombre_nucleo, x, ~nucleos[[.x]](.y))
) %>%
  ggplot(aes(x, y, color = nombre_nucleo)) +
    geom_line() +
    coord_fixed() -> fig2

ggsave("fig2_nucleos.png", fig2, width = 6, height = 3.5)
fig2
```

```{r}
estimar_densidad <- function(X, nombre_nucleo, h, x) {
    n <- length(X)
    nucleo <- nucleos[[nombre_nucleo]]   
    sum(nucleo((X - x) / h)) / (2 * n * h)
}
```

Estimamos la densidad con todos los nucleos definidos sobre una primera grilla que sencillamente recorre los valores entre el máximo y el mínimo del conjunto de datos. 

En este caso, el bucle `for` recorre la lista de valores tentativos de `h` provista y luego de realizar las estimaciones con ese `h` para cada núcleo produce un gráfico que sintetiza la información obtenida. 

```{r}
grilla <- min(df$elevacion):max(df$elevacion)

for (h in c(50, 200, 400)) {
    tibble(
        x = rep(grilla, length(names(nucleos))),
        nucleo = rep(names(nucleos), each = length(grilla)),
        densidad = map2_dbl(x, nucleo, function(x, nucleo) {
            estimar_densidad(df$elevacion, nucleo, h, x)
        })
    ) %>%
      ggplot() +
        aes(x, y = densidad, color = nucleo) +
        geom_line() +
        labs(title = glue('h = {h}')) +
        theme(
            axis.ticks.y = element_blank(),
            axis.text.y = element_blank()
        ) -> fig

    ggsave(glue('fig3_densidad_con_h_{h}.png'), fig, width = 7, height = 4)
    print(fig)
}
```

Elegimos el núcleo de Epanechnikov. Vamos a realizar la regresión no paramétrica con el método Nadaraya-Watson.


Además de definir este estimador, ensayamos también una función currificada `construir_estimador_de_error_NW_dado_h` que va a ser útil a la hora de realizar la validación del modelo, ya que permite fijar los conjuntos de datos y tomar `h` como única variable, encargándose la función de realizar un proceso de validación cruzada "Validación con un dato fuera" o *LOOCV* por sus siglas inglesas.

El costo del proceso es quizás excesivo pero optamos por usarlo en lugar del *K-Fold* porque garantiza estabilidad en los resultados y eso es fundamental a la hora de aclimatarse con una herramienta como esta, además de que la exigüidad de los datos lo hace posible.

```{r}
NUCLEO_ELEGIDO <- 'epanechnikov'

construir_estimador_NW <- function(X, Y, nombre_nucleo, h) {
    function(x) {
        nucleo <- nucleos[[nombre_nucleo]]   
        pesos <- nucleo((X - x) / h)
        weighted.mean(Y, pesos)
    }
}

construir_estimador_de_error_NW_dado_h <- function(X, Y, nombre_nucleo) {
    function(h) {
        estimador_iesimo <- function(i) { construir_estimador_NW(X[-i], Y[-i], nombre_nucleo, h) }
        estimadores <- map(seq_along(X), estimador_iesimo)
        predicciones <- pmap_dbl(
          list(estimador = estimadores, x = X),
          function(estimador, x) { estimador(x) }
        )
        return (mean((Y - predicciones)^2))
    }
}

estimador_de_error_NW_dado_h <-
  construir_estimador_de_error_NW_dado_h(df$elevacion, df$temp_anual, NUCLEO_ELEGIDO)
```

Intentamos utilizar `optimize` para encontrar el `h` que minimiza el error, pero el resultado varía según el rango que recibe. Sospechamos que la función se queda en una región "amesetada" alrededor del 370 y sólo puede llegar al verdadero mínimo (alrededor de $150$, como veremos más adelante) si restringimos el rango de valores de `h` en el que busca el extremo:
```{r}


resultado_1 <- optimize(estimador_de_error_NW_dado_h, c(-10, 1000))$minimum
resultado_2 <- optimize(estimador_de_error_NW_dado_h, c(-10, 250))$minimum # Rango restringido

print(glue('h con error mínimo según "optimize", sin restringir el rango = {round(resultado_1, 2)}'))
print(glue('h con error mínimo según "optimize", restringiendo el rango = {round(resultado_2, 2)}'))
```

Vamos a buscar el valor de `h` óptimo con el método Leave-one-out cross validation (LOOCV), que como mencionamos es parte de la implementación de `estimador_de_error_NW_segun_h`.

```{r}
valores_h <- c(seq(from = 40, to = 1000, by = 50), # Grilla gruesa
               seq(from = 40, to = 350, by = 10), # Grilla más fina en región candidata
               seq(from = 120, to = 190, by = 1)) # Grilla muy fina en el lo más hondo del valle

errores_segun_h <- tibble(
  h = sort(unique(valores_h)),
  error_nw = map_dbl(h, estimador_de_error_NW_dado_h)
)

MEJOR_H <- slice(errores_segun_h, which.min(error_nw))$h

errores_segun_h %>%
  filter(!is.na(error_nw)) %>%
  ggplot(aes(h, error_nw)) +
    geom_point(shape = 39, size = 3) +
    geom_line(alpha = .3) +
    geom_vline(xintercept = MEJOR_H, alpha = .6, linetype = 'dashed', color = 'ForestGreen') +
    annotate('text', x = MEJOR_H + 1, y = 0.9, label = glue('h = {MEJOR_H}'), color = 'ForestGreen') +
    labs(title = glue('Error mínimo del estimador N-W con h = {MEJOR_H}')) +
    ylab('Error cuadrado medio') +
    scale_x_continuous(breaks = c(seq(100, 200, 20), seq(0, 1000, 100))) +
    theme(axis.text.x = element_text(angle = 90),
          axis.ticks.y = element_blank()) -> fig

ggsave('fig4_error_NW_segun_h.png', fig, width = 7, height = 4)
fig
```

```{r}

# TODO: Seguir con esto! LOOCV con modelos lineal y cuadrático
# y luego comparar con mejor_error$error_nw

# Estimación paramétrica

lin <- lm(temp_anual ~ elevacion, df)
cuad <- lm(temp_anual ~ poly(elevacion, 2), df)

evaluar_modelo_lineal_LOOCV <- function(X,Y){
    estimador_iesimo <- function(i) {
        parametros <- unname(coefficients(
            lm(Y~X,list(X[-i],Y[-i]))
        ))
        ordenadaO <- parametros[[1]]
        pendiente <- parametros[[2]]
        return (
            function(x) {pendiente*x+ordenadaO}
            )

    }
    estimadores <- map(seq_along(X), estimador_iesimo)
    predicciones <- pmap_dbl(
          list(estimador = estimadores, x = X),
          function(estimador, x) { estimador(x) }
        )
        return (mean((Y - predicciones)^2))
    }

error_lin <- evaluar_modelo_lineal_LOOCV(df$elevacion,df$temp_anual)
error_lin
```



