---
title: "TP-3"
output: html_document
author: Barrera Borla, Berros, Duarte
---
# Preliminares

Cargamos las librerías del *pulcriverso*.

```{r inicialización}
library(tidyverse, quietly = TRUE)
library(glue)
```

Leemos los datos y realizamos, construimos un `data-frame` con nombres significativos y tipos adecuados a cada variable para, en base a él,  realizar el scatter-plot de las variables elevación y temperatura anual.

```{r ingreso de datos, cache=T}
df <- read_csv(
  file = "ortann.csv",
  col_types = cols(
    station = col_character(),
    latitude = col_double(),
    longitude = col_double(),
    elevation = col_integer(),
    tann = col_double()
  ))

nombres_castellano <- c(
  "station" = "estacion",
  "latitude" = "latitud",
  "longitude" = "longitud",
  "elevation" = "elevacion",
  "tann" = "temp_anual")

df <- plyr::rename(df, nombres_castellano)

df %>%
  ggplot(aes(x = elevacion, y = temp_anual)) +
  geom_point() -> fig1

ggsave("fig1_dispersion.png", fig1)
fig1
```

Siguiendo el criterio de trabajos precedentes, quitamos dos outliers del dataset.

```{r eliminación de atípicos}
df <- filter(df, elevacion < 1500)
```
# Estimación No Paramétrica

## Implementación del Estimador de Nadaraya Watson

### Definición de los Núcleos

Definimos los núcleos para la estimación no paramétrica.

```{r núcleos}
indicadora <- function(z) { ifelse(abs(z) <= 1, 1, 0) }

nucleos <- list(
    'uniforme' = function(z) { indicadora(z) * (1/2) },
    'triangular' = function(z) { indicadora(z) * (1 - abs(z)) },
    'epanechnikov' = function(z) { indicadora(z) * (3/4) * (1 - z^2) },
    'gaussiano' = function(z) { (1/sqrt(2 * pi)) * exp(-z^2/2) }
)
```

Y los graficamos para comprobar que actúan como esperamos.

```{r gráfico de los núcleos, cache=T}
rango <- seq(-1.1, 1.1, by = 0.01)

tibble(
  x = rep(rango, each = length(nucleos)),
  nombre_nucleo = rep(names(nucleos), length(rango)),
  y =  map2_dbl(nombre_nucleo, x, ~nucleos[[.x]](.y))
) %>%
  ggplot(aes(x, y, color = nombre_nucleo)) +
    geom_line() +
    coord_fixed() -> fig2

ggsave("fig2_nucleos.png", fig2, width = 6, height = 3.5)
fig2
```

### Estimador de la Densidad

```{r}
estimar_densidad <- function(X, nombre_nucleo, h, x) {
    n <- length(X)
    nucleo <- nucleos[[nombre_nucleo]]   
    sum(nucleo((X - x) / h)) / (2 * n * h)
}
```

Estimamos la densidad con todos los nucleos definidos sobre una primera grilla que sencillamente recorre los valores entre el máximo y el mínimo del conjunto de datos. 

En este caso, el bucle `for` recorre la lista de valores tentativos de `h` provista y luego de realizar las estimaciones con ese `h` para cada núcleo produce un gráfico que sintetiza la información obtenida. 

```{r}
grilla <- min(df$elevacion):max(df$elevacion)

for (h in c(50, 200, 400)) {
    tibble(
        x = rep(grilla, length(names(nucleos))),
        nucleo = rep(names(nucleos), each = length(grilla)),
        densidad = map2_dbl(x, nucleo, function(x, nucleo) {
            estimar_densidad(df$elevacion, nucleo, h, x)
        })
    ) %>%
      ggplot() +
        aes(x, y = densidad, color = nucleo) +
        geom_line() +
        labs(title = glue('h = {h}')) +
        theme(
            axis.ticks.y = element_blank(),
            axis.text.y = element_blank()
        ) -> fig

    ggsave(glue('fig3_densidad_con_h_{h}.png'), fig, width = 7, height = 4)
    print(fig)
}
```

### Implementación de la Regresión No Paramétrica

Elegimos el núcleo de Epanechnikov. Vamos a realizar la regresión no paramétrica con el método Nadaraya-Watson.


Además de definir este estimador, ensayamos también una función currificada `construir_estimador_de_error_NW_dado_h` que va a ser útil a la hora de realizar la validación del modelo, ya que permite fijar los conjuntos de datos y tomar `h` como única variable, encargándose la función de realizar un proceso de validación cruzada "Validación con un dato fuera" o *LOOCV* por sus siglas inglesas.

El costo del proceso es quizás excesivo pero optamos por usarlo en lugar del *K-Fold* porque garantiza estabilidad en los resultados y eso es fundamental a la hora de aclimatarse con una herramienta como esta, además de que la exigüidad de los datos lo hace posible.

```{r implementación de la regresión no paramétrica}
NUCLEO_ELEGIDO <- 'epanechnikov'

construir_estimador_NW <- function(X, Y, nombre_nucleo, h) {
    function(x) {
        nucleo <- nucleos[[nombre_nucleo]]   
        pesos <- nucleo((X - x) / h)
        weighted.mean(Y, pesos)
    }
}

construir_estimador_de_error_NW_dado_h <- function(X, Y, nombre_nucleo) {
    function(h) {
        estimador_iesimo <- function(i) { construir_estimador_NW(X[-i], Y[-i], nombre_nucleo, h) }
        estimadores <- map(seq_along(X), estimador_iesimo)
        predicciones <- pmap_dbl(
          list(estimador = estimadores, x = X),
          function(estimador, x) { estimador(x) }
        )
        return (mean((Y - predicciones)^2))
    }
}

estimador_de_error_NW_dado_h <-
  construir_estimador_de_error_NW_dado_h(df$elevacion, df$temp_anual, NUCLEO_ELEGIDO)
```

#### Buscamos una Referencia sobre el mejor *h*

Intentamos utilizar `optimize` para encontrar el `h` que minimiza el error, pero el resultado varía según el rango que recibe. Sospechamos que la función se queda en una región "amesetada" alrededor del 370 y sólo puede llegar al verdadero mínimo (alrededor de $150$, como veremos más adelante) si restringimos el rango de valores de `h` en el que busca el extremo:
```{r}


resultado_1 <- optimize(estimador_de_error_NW_dado_h, c(-10, 1000))$minimum
resultado_2 <- optimize(estimador_de_error_NW_dado_h, c(-10, 250))$minimum # Rango restringido

print(glue('h con error mínimo según "optimize", sin restringir el rango = {round(resultado_1, 2)}'))
print(glue('h con error mínimo según "optimize", restringiendo el rango = {round(resultado_2, 2)}'))
```

#### Buscamos el mejor *h* con una implementación propia

Vamos a buscar el valor de *h* óptimo con el método Leave-one-out cross validation (LOOCV), que como mencionamos es parte de la implementación de `estimador_de_error_NW_segun_h`.

```{r LOOCV para NW, cache=T }
valores_h <- c(seq(from = 40, to = 1000, by = 50), # Grilla gruesa
               seq(from = 40, to = 350, by = 10), # Grilla más fina en región candidata
               seq(from = 120, to = 190, by = 1)) # Grilla muy fina en el lo más hondo del valle

errores_segun_h <- tibble(
  h = sort(unique(valores_h)),
  error_nw = map_dbl(h, estimador_de_error_NW_dado_h)
)

MEJOR_H <- slice(errores_segun_h, which.min(error_nw))$h

errores_segun_h %>%
  filter(!is.na(error_nw)) %>%
  ggplot(aes(h, error_nw)) +
    geom_point(shape = 39, size = 3) +
    geom_line(alpha = .3) +
    geom_vline(xintercept = MEJOR_H, alpha = .6, linetype = 'dashed', color = 'ForestGreen') +
    annotate('text', x = MEJOR_H + 1, y = 0.9, label = glue('h = {MEJOR_H}'), color = 'ForestGreen') +
    labs(title = glue('Error mínimo del estimador N-W con h = {MEJOR_H}')) +
    ylab('Error cuadrado medio') +
    scale_x_continuous(breaks = c(seq(100, 200, 20), seq(0, 1000, 100))) +
    theme(axis.text.x = element_text(angle = 90),
          axis.ticks.y = element_blank()) -> fig

ggsave('fig4_error_NW_segun_h.png', fig, width = 7, height = 4)
fig

modelo_no_parametrico <- construir_estimador_NW( df$elevacion, df$temp_anual, NUCLEO_ELEGIDO, 155 )

```

#### Estimamos por Otro Núcleo

De esta forma, nos cercioramos de que el resultado obtenido es representativo. Seguimos un procedimiento similar (cambiando la variable `NUCLEO_ELEGIDO`) y hallamos que en ese caso el *h* óptimo es 98,87. Como se ve el cambio de nucleo no impacta sobre el rendimiento de la regresión.   

```{r modelo Gaussiano, cache=T}

modelo_no_parametrico2 <- construir_estimador_NW( df$elevacion, df$temp_anual, 'gaussiano', 98.87 )

estimador_de_error_NW_dado_h2 <-
  construir_estimador_de_error_NW_dado_h(df$elevacion, df$temp_anual, 'gaussiano')

error_gaussiano <- estimador_de_error_NW_dado_h2(98.87)
error_gaussiano
```
# Estimación paramétrica

Vamos a comprar nuestro modelo no paramétrico con sus antagonistas paramétricos ajustando varios polinomios por `lm` y obteniendo su error por procedimientos similares. 
Primero vamos a definir una función `polinomio_dados_coeficientes` para poder aprovechar la salida del comando `lm`.

```{r estimaciones por lm, cache=T}

polinomio_dados_coeficientes <- function(coeficientes) {
  polinomio <- function(x) {
    sum(
      pmap_dbl(
        list( i = seq_along(coeficientes), coeficiente = coeficientes ),
        function(i,coeficiente) {coeficiente * x^(i - 1)}
      )
    )
  }
  return(polinomio)
}  

evaluar_modelo_lineal_LOOCV <- function(X,Y,grado){
  estimador_iesimo <- function(i) {
    coeficientes <- coefficients(
      lm(Y~poly(X,degree=grado,raw=T),list(X[-i],Y[-i]))
      )
    return ( polinomio_dados_coeficientes(coeficientes) )

  }
  estimadores <- map(seq_along(X), estimador_iesimo)
  predicciones <- pmap_dbl(
    list(estimador = estimadores, x = X),
    function(estimador, x) { estimador(x) }
  )
  return (mean((Y - predicciones)^2))
}

error_lin <- evaluar_modelo_lineal_LOOCV(df$elevacion,df$temp_anual,1)
error_cuad <- evaluar_modelo_lineal_LOOCV(df$elevacion,df$temp_anual,2)
error_NW <- estimador_de_error_NW_dado_h(155)

error_lin
error_cuad
error_NW

modelo_lineal <- polinomio_dados_coeficientes (unname ( coefficients (
  lm (temp_anual ~ poly(elevacion, degree = 1,raw=T), list(elevacion = df$elevacion,temp_anual = df$temp_anual))
  ) ) )

modelo_cuadratico <- polinomio_dados_coeficientes (unname ( coefficients (
  lm (temp_anual ~ poly(elevacion, degree = 2,raw=T),list(elevacion = df$elevacion,temp_anual = df$temp_anual))
  ) ) )

modelo_grado_alto <- polinomio_dados_coeficientes (unname ( coefficients (
  lm (temp_anual ~ poly(elevacion, degree = 13,raw=T),list(elevacion = df$elevacion,temp_anual = df$temp_anual))
  ) ) )

error_grado_alto <- evaluar_modelo_lineal_LOOCV(df$elevacion,df$temp_anual,13)
error_grado_alto

error_polinomios <- tibble(
  grado = seq(1,20),
  error = map_dbl(grado,~evaluar_modelo_lineal_LOOCV(df$elevacion,df$temp_anual,.x))
)

error_polinomios
```


```{r gráfico encimado de los modelos, cache=T}

rango <- seq(min(df$elevacion),max(df$elevacion), by = 1)

modelos <- list(
  'lineal' = modelo_lineal,
  'cuadrático' = modelo_cuadratico,
  'grado trece' = modelo_grado_alto,
  'no paramétrico' = modelo_no_parametrico
)

tabla_modelos <- tibble(
  x = rep(rango, each = length(modelos)),
  nombre_modelo = rep(names(modelos), length(rango)),
  y =  map2_dbl(nombre_modelo, x, ~modelos[[.x]](.y))
) 
  ggplot(tabla_modelos,aes(x, y)) +
    geom_line(aes(color=nombre_modelo)) +
    geom_point(data = df, aes( x = elevacion, y = temp_anual)) -> figComparacion

ggsave("figComparacion.png", figComparacion, width = 10, height = 10)
figComparacion
```
#Tabla comparativa de los errores obtenidos 

Modelo | Error
------------------------------|----------------------------
Lineal                        | `r error_lin`
Cuadrático                      | `r error_cuad`
Grado 13                        | `r error_grado_alto`
No Paramétrico Epanechnikov       | `r error_NW`
No Paramétrico Gaussiano          | `r error_gaussiano`

# Conclusiones

 Decepcionantemente y a pesar de nuestro esfuerzo, los polinomios ajustados por la función implementada `lm` ofrecen un error menor para la validación cruzada a partir de un grado tan bajo como 2 y se perfilan con una eficiencia muy superior en este caso.


 Esto se explica considerando que los mismos cuando existen siempre ofrecen el mínimo error posible, cosa que la regresión no paramétrica no puede garantizar.


  La bibliografía hace constar además dos datos relevantes en este sentido: el primero es que en general las regresiones no paramétricas requieren grandes conjuntos de datos para alcanzar su mejor desempeño (aunque en particular las que usan nucleos morigeran esta necesidad) y el segundo es que la fortaleza de este sistema es adaptarse a modelos que requieren funciones inusuales, cosa que aparentemente no sucede en este caso. 
 
 Queda pendiente repetir este proceso para algun conjunto de datos más grande y errático. 

